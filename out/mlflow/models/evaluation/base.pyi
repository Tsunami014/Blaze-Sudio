import mlflow
from _typeshed import Incomplete
from abc import ABCMeta, abstractmethod
from mlflow.entities import RunTag as RunTag
from mlflow.exceptions import MlflowException as MlflowException
from mlflow.models.evaluation.validation import MetricThreshold as MetricThreshold, ModelValidationFailedException as ModelValidationFailedException
from mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE as INVALID_PARAMETER_VALUE
from mlflow.tracking.client import MlflowClient as MlflowClient
from mlflow.utils.annotations import experimental as experimental
from mlflow.utils.file_utils import TempDir as TempDir
from mlflow.utils.proto_json_utils import NumpyEncoder as NumpyEncoder
from mlflow.utils.string_utils import generate_feature_name_if_not_string as generate_feature_name_if_not_string
from typing import Any

class EvaluationArtifact(metaclass=ABCMeta):
    def __init__(self, uri, content: Incomplete | None = None) -> None: ...
    @property
    def content(self): ...
    @property
    def uri(self) -> str: ...

class EvaluationResult:
    def __init__(self, metrics, artifacts, baseline_model_metrics: Incomplete | None = None) -> None: ...
    @classmethod
    def load(cls, path): ...
    def save(self, path) -> None: ...
    @property
    def metrics(self) -> dict[str, Any]: ...
    @property
    def artifacts(self) -> dict[str, 'mlflow.models.EvaluationArtifact']: ...
    @property
    def baseline_model_metrics(self) -> dict[str, Any]: ...

class EvaluationDataset:
    NUM_SAMPLE_ROWS_FOR_HASH: int
    SPARK_DATAFRAME_LIMIT: int
    def __init__(self, data, *, targets, name: Incomplete | None = None, path: Incomplete | None = None, feature_names: Incomplete | None = None) -> None: ...
    @property
    def feature_names(self): ...
    @property
    def features_data(self): ...
    @property
    def labels_data(self): ...
    @property
    def name(self): ...
    @property
    def path(self): ...
    @property
    def hash(self): ...
    def __hash__(self): ...
    def __eq__(self, other): ...

class ModelEvaluator(metaclass=ABCMeta):
    @abstractmethod
    def can_evaluate(self, *, model_type, evaluator_config, **kwargs) -> bool: ...
    @abstractmethod
    def evaluate(self, *, model, model_type, dataset, run_id, evaluator_config, custom_metrics: Incomplete | None = None, baseline_model: Incomplete | None = None, **kwargs): ...

def list_evaluators(): ...
def evaluate(model: str | mlflow.pyfunc.PyFuncModel, data, *, targets, model_type: str, dataset_name: Incomplete | None = None, dataset_path: Incomplete | None = None, feature_names: list = None, evaluators: Incomplete | None = None, evaluator_config: Incomplete | None = None, custom_metrics: Incomplete | None = None, validation_thresholds: Incomplete | None = None, baseline_model: Incomplete | None = None, env_manager: str = 'local'): ...
