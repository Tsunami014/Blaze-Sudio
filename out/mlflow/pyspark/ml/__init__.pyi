from _typeshed import Incomplete
from mlflow.entities import Metric as Metric, Param as Param
from mlflow.exceptions import MlflowException as MlflowException
from mlflow.tracking.client import MlflowClient as MlflowClient
from mlflow.utils.autologging_utils import INPUT_EXAMPLE_SAMPLE_ROWS as INPUT_EXAMPLE_SAMPLE_ROWS, autologging_integration as autologging_integration, get_method_call_arg_value as get_method_call_arg_value, resolve_input_example_and_signature as resolve_input_example_and_signature, safe_patch as safe_patch
from mlflow.utils.file_utils import TempDir as TempDir
from mlflow.utils.mlflow_tags import MLFLOW_AUTOLOGGING as MLFLOW_AUTOLOGGING, MLFLOW_PARENT_RUN_ID as MLFLOW_PARENT_RUN_ID
from mlflow.utils.rest_utils import MlflowHostCreds as MlflowHostCreds, augmented_raise_for_status as augmented_raise_for_status, http_request as http_request
from mlflow.utils.time_utils import get_current_time_millis as get_current_time_millis
from mlflow.utils.validation import MAX_ENTITY_KEY_LENGTH as MAX_ENTITY_KEY_LENGTH, MAX_PARAMS_TAGS_PER_BATCH as MAX_PARAMS_TAGS_PER_BATCH, MAX_PARAM_VAL_LENGTH as MAX_PARAM_VAL_LENGTH
from typing import NamedTuple

AUTOLOGGING_INTEGRATION_NAME: str

class _AutologgingEstimatorMetadata(NamedTuple):
    hierarchy: Incomplete
    uid_to_indexed_name_map: Incomplete
    param_search_estimators: Incomplete

class _AutologgingMetricsManager:
    def __init__(self) -> None: ...
    def should_log_post_training_metrics(self): ...
    def disable_log_post_training_metrics(self): ...
    @staticmethod
    def get_run_id_for_model(model): ...
    @staticmethod
    def is_metric_value_loggable(metric_value): ...
    def register_model(self, model, run_id) -> None: ...
    @staticmethod
    def gen_name_with_index(name, index): ...
    def register_prediction_input_dataset(self, model, eval_dataset): ...
    def register_prediction_result(self, run_id, eval_dataset_name, predict_result) -> None: ...
    def get_run_id_and_dataset_name_for_evaluator_call(self, pred_result_dataset): ...
    def gen_evaluator_info(self, evaluator): ...
    def register_evaluator_call(self, run_id, metric_name, dataset_name, evaluator_info): ...
    def log_post_training_metric(self, run_id, key, value): ...

def autolog(log_models: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, log_post_training_metrics: bool = True, registered_model_name: Incomplete | None = None, log_input_examples: bool = False, log_model_signatures: bool = True): ...
