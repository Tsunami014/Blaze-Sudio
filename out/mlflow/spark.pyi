from _typeshed import Incomplete
from mlflow import environment_variables as environment_variables, mleap as mleap, pyfunc as pyfunc
from mlflow.environment_variables import MLFLOW_DFS_TMP as MLFLOW_DFS_TMP
from mlflow.exceptions import MlflowException as MlflowException
from mlflow.models import Model as Model
from mlflow.models.model import MLMODEL_FILE_NAME as MLMODEL_FILE_NAME
from mlflow.models.signature import ModelSignature as ModelSignature
from mlflow.models.utils import ModelInputExample as ModelInputExample
from mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE as INVALID_PARAMETER_VALUE
from mlflow.store.artifact.databricks_artifact_repo import DatabricksArtifactRepository as DatabricksArtifactRepository
from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository as ModelsArtifactRepository
from mlflow.store.artifact.runs_artifact_repo import RunsArtifactRepository as RunsArtifactRepository
from mlflow.tracking._model_registry import DEFAULT_AWAIT_MAX_SLEEP_SECONDS as DEFAULT_AWAIT_MAX_SLEEP_SECONDS
from mlflow.utils import databricks_utils as databricks_utils
from mlflow.utils.autologging_utils import autologging_integration as autologging_integration, safe_patch as safe_patch
from mlflow.utils.docstring_utils import LOG_MODEL_PARAM_DOCS as LOG_MODEL_PARAM_DOCS, format_docstring as format_docstring
from mlflow.utils.file_utils import TempDir as TempDir, write_to as write_to
from mlflow.utils.uri import append_to_uri_path as append_to_uri_path, dbfs_hdfs_uri_to_fuse_path as dbfs_hdfs_uri_to_fuse_path, get_databricks_profile_uri_from_artifact_uri as get_databricks_profile_uri_from_artifact_uri, is_databricks_acled_artifacts_uri as is_databricks_acled_artifacts_uri, is_local_uri as is_local_uri, is_valid_dbfs_uri as is_valid_dbfs_uri

FLAVOR_NAME: str

def get_default_pip_requirements(): ...
def get_default_conda_env(): ...
def log_model(spark_model, artifact_path, conda_env: Incomplete | None = None, code_paths: Incomplete | None = None, dfs_tmpdir: Incomplete | None = None, sample_input: Incomplete | None = None, registered_model_name: Incomplete | None = None, signature: ModelSignature = None, input_example: ModelInputExample = None, await_registration_for=..., pip_requirements: Incomplete | None = None, extra_pip_requirements: Incomplete | None = None): ...

class _HadoopFileSystem:
    def __init__(self) -> None: ...
    @classmethod
    def copy_to_local_file(cls, src, dst, remove_src) -> None: ...
    @classmethod
    def copy_from_local_file(cls, src, dst, remove_src) -> None: ...
    @classmethod
    def qualified_local_path(cls, path): ...
    @classmethod
    def maybe_copy_from_local_file(cls, src, dst): ...
    @classmethod
    def maybe_copy_from_uri(cls, src_uri, dst_path, local_model_path: Incomplete | None = None): ...
    @classmethod
    def delete(cls, path) -> None: ...
    @classmethod
    def is_filesystem_available(cls, scheme): ...

def save_model(spark_model, path, mlflow_model: Incomplete | None = None, conda_env: Incomplete | None = None, code_paths: Incomplete | None = None, dfs_tmpdir: Incomplete | None = None, sample_input: Incomplete | None = None, signature: ModelSignature = None, input_example: ModelInputExample = None, pip_requirements: Incomplete | None = None, extra_pip_requirements: Incomplete | None = None): ...
def load_model(model_uri, dfs_tmpdir: Incomplete | None = None, dst_path: Incomplete | None = None): ...

class _PyFuncModelWrapper:
    spark: Incomplete
    spark_model: Incomplete
    def __init__(self, spark, spark_model) -> None: ...
    def predict(self, pandas_df): ...

def autolog(disable: bool = False, silent: bool = False) -> None: ...
