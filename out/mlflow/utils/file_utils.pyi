from _typeshed import Incomplete
from collections.abc import Generator
from mlflow.entities import FileInfo as FileInfo
from mlflow.exceptions import MissingConfigException as MissingConfigException
from mlflow.utils import merge_dicts as merge_dicts
from mlflow.utils.os import is_windows as is_windows
from mlflow.utils.process import cache_return_value_per_process as cache_return_value_per_process
from mlflow.utils.rest_utils import augmented_raise_for_status as augmented_raise_for_status, cloud_storage_http_request as cloud_storage_http_request
from yaml import SafeLoader as YamlSafeLoader

ENCODING: str

def is_directory(name): ...
def is_file(name): ...
def exists(name): ...
def list_all(root, filter_func=..., full_path: bool = False): ...
def list_subdirs(dir_name, full_path: bool = False): ...
def list_files(dir_name, full_path: bool = False): ...
def find(root, name, full_path: bool = False): ...
def mkdir(root, name: Incomplete | None = None): ...
def make_containing_dirs(path) -> None: ...
def write_yaml(root, file_name, data, overwrite: bool = False, sort_keys: bool = True) -> None: ...
def read_yaml(root, file_name): ...

class UniqueKeyLoader(YamlSafeLoader):
    def construct_mapping(self, node, deep: bool = False): ...

def render_and_merge_yaml(root, template_name, context_name): ...
def read_parquet_as_pandas_df(data_parquet_path: str): ...
def write_pandas_df_as_parquet(df, data_parquet_path: str): ...

class TempDir:
    def __init__(self, chdr: bool = False, remove_on_exit: bool = True) -> None: ...
    def __enter__(self): ...
    def __exit__(self, tp: type[BaseException] | None, val: BaseException | None, traceback: types.TracebackType | None) -> None: ...
    def path(self, *path): ...

def read_file_lines(parent_path, file_name): ...
def read_file(parent_path, file_name): ...
def get_file_info(path, rel_path): ...
def get_relative_path(root_path, target_path): ...
def mv(target, new_parent) -> None: ...
def write_to(filename, data) -> None: ...
def append_to(filename, data) -> None: ...
def make_tarfile(output_filename, source_dir, archive_name, custom_filter: Incomplete | None = None): ...
def get_parent_dir(path): ...
def relative_path_to_artifact_path(path): ...
def path_to_local_file_uri(path): ...
def path_to_local_sqlite_uri(path): ...
def local_file_uri_to_path(uri): ...
def get_local_path_or_none(path_or_uri): ...
def yield_file_in_chunks(file, chunk_size: int = 100000000) -> Generator[Incomplete, None, None]: ...
def download_file_using_http_uri(http_uri, download_path, chunk_size: int = 100000000, headers: Incomplete | None = None) -> None: ...
def get_or_create_tmp_dir(): ...
def get_or_create_nfs_tmp_dir(): ...
def write_spark_dataframe_to_parquet_on_local_disk(spark_df, output_path) -> None: ...
