from _typeshed import Incomplete
from mlflow.exceptions import MlflowException as MlflowException
from mlflow.models import Model as Model, ModelInputExample as ModelInputExample, ModelSignature as ModelSignature
from mlflow.models.model import MLMODEL_FILE_NAME as MLMODEL_FILE_NAME
from mlflow.models.utils import PyFuncInput as PyFuncInput, PyFuncOutput as PyFuncOutput
from mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE as INVALID_PARAMETER_VALUE, RESOURCE_DOES_NOT_EXIST as RESOURCE_DOES_NOT_EXIST
from mlflow.pyfunc.model import PythonModel as PythonModel, PythonModelContext as PythonModelContext, get_default_conda_env as get_default_conda_env, get_default_pip_requirements as get_default_pip_requirements
from mlflow.tracking._model_registry import DEFAULT_AWAIT_MAX_SLEEP_SECONDS as DEFAULT_AWAIT_MAX_SLEEP_SECONDS
from mlflow.utils import PYTHON_VERSION as PYTHON_VERSION, find_free_port as find_free_port, get_major_minor_py_version as get_major_minor_py_version
from mlflow.utils.annotations import deprecated as deprecated, experimental as experimental
from mlflow.utils.databricks_utils import is_in_databricks_runtime as is_in_databricks_runtime
from mlflow.utils.docstring_utils import LOG_MODEL_PARAM_DOCS as LOG_MODEL_PARAM_DOCS, format_docstring as format_docstring
from mlflow.utils.file_utils import get_or_create_nfs_tmp_dir as get_or_create_nfs_tmp_dir, get_or_create_tmp_dir as get_or_create_tmp_dir, write_to as write_to
from mlflow.utils.nfs_on_spark import get_nfs_cache_root_dir as get_nfs_cache_root_dir
from mlflow.utils.process import cache_return_value_per_process as cache_return_value_per_process
from mlflow.utils.uri import append_to_uri_path as append_to_uri_path
from typing import Any

FLAVOR_NAME: str
MAIN: str
CODE: str
DATA: str
ENV: str
PY_VERSION: str

def add_to_model(model, loader_module, data: Incomplete | None = None, code: Incomplete | None = None, env: Incomplete | None = None, **kwargs): ...

class PyFuncModel:
    def __init__(self, model_meta: Model, model_impl: Any, predict_fn: str = 'predict') -> None: ...
    def predict(self, data: PyFuncInput) -> PyFuncOutput: ...
    def unwrap_python_model(self): ...
    def __eq__(self, other): ...
    @property
    def metadata(self): ...

def load_model(model_uri: str, suppress_warnings: bool = False, dst_path: str = None) -> PyFuncModel: ...

class _ServedPyFuncModel(PyFuncModel):
    def __init__(self, model_meta: Model, client: Any, server_pid: int, env_manager: str) -> None: ...
    def predict(self, data): ...
    @property
    def pid(self): ...
    @property
    def env_manager(self): ...

def get_model_dependencies(model_uri, format: str = 'pip'): ...
def load_pyfunc(model_uri, suppress_warnings: bool = False): ...
def spark_udf(spark, model_uri, result_type: str = 'double', env_manager: str = 'local'): ...
def save_model(path, loader_module: Incomplete | None = None, data_path: Incomplete | None = None, code_path: Incomplete | None = None, conda_env: Incomplete | None = None, mlflow_model: Incomplete | None = None, python_model: Incomplete | None = None, artifacts: Incomplete | None = None, signature: ModelSignature = None, input_example: ModelInputExample = None, pip_requirements: Incomplete | None = None, extra_pip_requirements: Incomplete | None = None, **kwargs): ...
def log_model(artifact_path, loader_module: Incomplete | None = None, data_path: Incomplete | None = None, code_path: Incomplete | None = None, conda_env: Incomplete | None = None, python_model: Incomplete | None = None, artifacts: Incomplete | None = None, registered_model_name: Incomplete | None = None, signature: ModelSignature = None, input_example: ModelInputExample = None, await_registration_for=..., pip_requirements: Incomplete | None = None, extra_pip_requirements: Incomplete | None = None): ...

loader_template: str
